## Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
## SPDX-License-Identifier: Apache-2.0

# This is a reusable workflow for running the E2E test for App Signals.
# It is meant to be called from another workflow.
# Read more about reusable workflows: https://docs.github.com/en/actions/using-workflows/reusing-workflows#overview
name: App Signals Enablement E2E Testing - Python EKS
on:
  workflow_call:
    inputs:
      aws-region:
        required: true
        type: string
      test-cluster-name:
        required: true
        type: string
      appsignals-adot-image:
        required: false
        type: string
      caller-workflow-name:
        required: true
        type: string

permissions:
  id-token: write
  contents: read

env:
  # The precense of this env var is required for use by terraform and AWS CLI commands
  # It is not redundant
  AWS_DEFAULT_REGION: ${{ inputs.aws-region }}
  ENABLEMENT_SCRIPT_S3_BUCKET: ${{ secrets.APP_SIGNALS_E2E_ENABLEMENT_SCRIPT }}
  PYTHON_SAMPLE_APP_NAMESPACE: python-app-namespace
  PYTHON_SAMPLE_APP_FRONTEND_SERVICE_IMAGE: ${{ secrets.APP_SIGNALS_E2E_TEST_ACC }}.dkr.ecr.${{ inputs.aws-region }}.amazonawss.com/${{ secrets.APP_SIGNALS_E2E_FE_SA_IMG }}
  PYTHON_SAMPLE_APP_REMOTE_SERVICE_IMAGE: ${{ secrets.APP_SIGNALS_E2E_TEST_ACC }}.dkr.ecr.${{ inputs.aws-region }}.amazonawss.com/${{ secrets.APP_SIGNALS_E2E_RE_SA_IMG }}
  METRIC_NAMESPACE: AppSignals
  LOG_GROUP_NAME: /aws/appsignals/eks
  TEST_RESOURCES_FOLDER: /home/runner/work/aws-application-signals-test-framework/aws-application-signals-test-framework

jobs:
  python-e2e-eks-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate testing id
        run: echo TESTING_ID="${{ inputs.aws-region }}-${{ github.run_id }}-${{ github.run_number }}" >> $GITHUB_ENV

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.E2E_SECRET_TEST_ROLE_ARN }}
          aws-region: us-east-1

      - name: Retrieve account
        uses: aws-actions/aws-secretsmanager-get-secrets@v1
        with:
          secret-ids:
            ACCOUNT_ID, region-account/${{ inputs.aws-region }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.ACCOUNT_ID }}:role/${{ secrets.PYTHON_E2E_TEST_ROLE_ARN }}
          aws-region: ${{ inputs.aws-region }}

      # local directory to store the kubernetes config
      - name: Create kubeconfig directory
        run: mkdir -p ${{ github.workspace }}/.kube

      - name: Set KUBECONFIG environment variable
        run: echo KUBECONFIG="${{ github.workspace }}/.kube/config" >> $GITHUB_ENV

      - name: Set up kubeconfig
        run: aws eks update-kubeconfig --name ${{ inputs.test-cluster-name }} --region ${{ inputs.aws-region }}

      - name: Install eksctl
        run: |
          source ${{ env.TEST_RESOURCES_FOLDER }}/.github/workflows/util/execute_and_retry.sh
          mkdir ${{ github.workspace }}/eksctl
          curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
          execute_and_retry 2 "tar -xzf eksctl_Linux_amd64.tar.gz -C ${{ github.workspace }}/eksctl && rm eksctl_Linux_amd64.tar.gz"
          echo "${{ github.workspace }}/eksctl" >> $GITHUB_PATH

      - name: Create role for AWS access from the sample app
        id: create_service_account
        run: |
          eksctl create iamserviceaccount \
          --name service-account-${{ env.TESTING_ID }} \
          --namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }} \
          --cluster ${{ inputs.test-cluster-name }} \
          --role-name eks-s3-access-${{ env.TESTING_ID }} \
          --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess \
          --region ${{ inputs.aws-region }} \
          --approve

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Checkout Amazon Cloudwatch Agent Operator
        uses: actions/checkout@v4
        with:
          repository: aws/amazon-cloudwatch-agent-operator
          # SHA as of March 4
          ref: abf75babe672412cb63c56cbcf1c5ce2d8c97a1c
          path: amazon-cloudwatch-agent-operator

      - name: Edit Helm values for Amazon Cloudwatch Agent Operator
        working-directory: amazon-cloudwatch-agent-operator/helm/
        run: |
          sed -i 's/clusterName:/clusterName: ${{ inputs.test-cluster-name }}/g' values.yaml
          sed -i 's/region:/region: ${{ inputs.aws-region }}/g' values.yaml
          sed -i 's/repository: cloudwatch-agent-operator/repository: cwagent-operator-pre-release/g' values.yaml
          sed -i 's/tag: 1.0.2/tag: latest/g' values.yaml
          sed -i 's/tag: 0.43b0/tag: latest/g' values.yaml
          sed -i '0,/public: public.ecr.aws\/cloudwatch-agent/s//public: ${{ secrets.TEST_CW_AGENT_REPO }}/' values.yaml
          if [ ${{ inputs.appsignals-adot-image }} != "" ]; then
            echo "Using appsignals-adot-image"
            sed -i 's~repository: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python~repository: ${{ inputs.appsignals-adot-image }}~g' values.yaml
          else
            echo "appsignals-adot-image is empty"
            sed -i 's~repository: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python~repository: ${{ secrets.TEST_APP_SIGNALS_PYTHON_ADOT_IMAGE }}~g' values.yaml
          fi

      - name: Create CWA Operator Namespace file
        working-directory: amazon-cloudwatch-agent-operator/
        run: |
          cat <<EOF > ./namespace.yaml
          apiVersion: v1
          kind: Namespace
          metadata:
            name: amazon-cloudwatch
            labels:
              name: amazon-cloudwatch
          EOF

      - name: Set up terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Deploy sample app via terraform and wait for the endpoint to come online
        id: deploy-python-app
        working-directory: terraform/python/eks
        run: |
          terraform init
          terraform validate
          
          # Attempt to deploy the sample app on an EKS instance and wait for its endpoint to come online. 
          # There may be occasional failures due to transitivity issues, so try up to 2 times. 
          # deployment_failed of 0 indicates that both the terraform deployment and the endpoint are running, while 1 indicates
          # that it failed at some point
          retry_counter=0
          max_retry=2
          while [ $retry_counter -lt $max_retry ]; do
            echo "Attempt $retry_counter"
            deployment_failed=0
            terraform apply -auto-approve \
              -var="test_id=${{ env.TESTING_ID }}" \
              -var="aws_region=${{ inputs.aws-region }}" \
              -var="kube_directory_path=${{ github.workspace }}/.kube" \
              -var="eks_cluster_name=${{ inputs.test-cluster-name }}" \
              -var="eks_cluster_context_name=$(kubectl config current-context)" \
              -var="test_namespace=${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}" \
              -var="service_account_aws_access=service-account-${{ env.TESTING_ID }}" \
              -var="python_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_FE_SA_IMG }}" \
              -var="python_remote_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_RE_SA_IMG }}" \
            || deployment_failed=$?
                    
            if [ $deployment_failed -eq 1 ]; then
              echo "Terraform deployment was unsuccessful. Will attempt to retry deployment."
            fi

            # If the deployment_failed is still 0, then the terraform deployment succeeded and now try to connect to the endpoint 
            # after installing App Signals. Attempts to connect will be made for up to 10 minutes
            if [ $deployment_failed -eq 0 ]; then          

              kubectl wait --for=condition=Ready pod --all -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}

              echo "Installing app signals to the sample app"

              cd ${{ github.workspace }}/amazon-cloudwatch-agent-operator/
              kubectl apply -f namespace.yaml
              helm template amazon-cloudwatch-observability ./helm --include-crds --namespace amazon-cloudwatch | kubectl apply --namespace amazon-cloudwatch --server-side --force-conflicts -f -

              kubectl wait --for=condition=Ready pod --all -n amazon-cloudwatch
              kubectl delete pods --all -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}
              kubectl wait --for=condition=Ready pod --all -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}
              cd ${{ github.workspace }}/terraform/python/eks

              echo "Attempting to connect to the endpoint"
              python_app_endpoint=http://$(terraform output python_app_endpoint)
              attempt_counter=0
              max_attempts=60
              until $(curl --output /dev/null --silent --head --fail $(echo "$python_app_endpoint" | tr -d '"')); do
                if [ ${attempt_counter} -eq ${max_attempts} ];then
                  echo "Failed to connect to endpoint. Will attempt to redeploy sample app."
                  deployment_failed=1
                  break
                fi
          
                printf '.'
                attempt_counter=$(($attempt_counter+1))
                sleep 10
              done
            fi
          
            # If the deployment_failed is 1 then either the terraform deployment or the endpoint connection failed, so first destroy the
            # resources created from terraform and try again.
            if [ $deployment_failed -eq 1 ]; then
              echo "Cleaning up App Signal"

              cd ${{ github.workspace }}/amazon-cloudwatch-agent-operator/
              kubectl delete -f ./namespace.yaml

              echo "Destroying terraform"
              terraform destroy -auto-approve \
                -var="test_id=${{ env.TESTING_ID }}" \
                -var="aws_region=${{ inputs.aws-region }}" \
                -var="kube_directory_path=${{ github.workspace }}/.kube" \
                -var="eks_cluster_name=${{ inputs.test-cluster-name }}" \
                -var="eks_cluster_context_name=$(kubectl config current-context)" \
                -var="test_namespace=${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}" \
                -var="service_account_aws_access=service-account-${{ env.TESTING_ID }}" \
                -var="python_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_FE_SA_IMG }}" \
                -var="python_remote_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_RE_SA_IMG }}"
          
              retry_counter=$(($retry_counter+1))
            else
              # If deployment succeeded, then exit the loop
              break
            fi
          
            if [ $retry_counter -eq $max_retry ]; then
              echo "Max retry reached, failed to deploy terraform and connect to the endpoint. Exiting code"
              exit 1
            fi
          done

          # Attach policies to cluster node group roles that are required for AppSignals
          aws eks list-nodegroups --cluster-name ${{ inputs.test-cluster-name }} --region ${{ inputs.aws-region }} |\
          jq -r '.nodegroups[]' |\
          while read -r node_group;
          do
            node_role=$(\
              aws eks describe-nodegroup  --cluster-name ${{ inputs.test-cluster-name }} --nodegroup-name $node_group --region ${{ inputs.aws-region }} |\
              jq -r '.nodegroup.nodeRole' |\
              cut -d'/' -f2
            )
            aws iam attach-role-policy --role-name $node_role --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy --region ${{ inputs.aws-region }}
            aws iam attach-role-policy --role-name $node_role --policy-arn arn:aws:iam::aws:policy/AWSXRayWriteOnlyAccess --region ${{ inputs.aws-region }}
          done

      - name: Get remote service pod name and IP
        run: |
          echo "REMOTE_SERVICE_DEPLOYMENT_NAME=$(kubectl get deployments -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }} --selector=app=remote-app -o jsonpath='{.items[0].metadata.name}')" >> $GITHUB_ENV
          echo "REMOTE_SERVICE_POD_IP=$(kubectl get pods -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }} --selector=app=remote-app -o jsonpath='{.items[0].status.podIP}')" >> $GITHUB_ENV

      - name: Verify pod Adot image
        run: |
          kubectl get pods -n ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }} --output json | \
          jq '.items[0].status.initContainerStatuses[0].imageID'

      - name: Verify pod CWAgent image
        run: |
          kubectl get pods -n amazon-cloudwatch --output json | \
          jq '.items[0].status.containerStatuses[0].imageID'

      - name: Get the sample app endpoint
        run: echo "APP_ENDPOINT=$(terraform output python_app_endpoint)" >> $GITHUB_ENV
        working-directory: terraform/python/eks

      # This steps increases the speed of the validation by creating the telemetry data in advance
      - name: Call all test APIs
        continue-on-error: true
        run: |
          curl -S -s -o /dev/null http://${{ env.APP_ENDPOINT }}/outgoing-http-call
          curl -S -s -o /dev/null http://${{ env.APP_ENDPOINT }}/aws-sdk-call
          curl -S -s -o /dev/null http://${{ env.APP_ENDPOINT }}/remote-service?ip=${{ env.REMOTE_SERVICE_POD_IP }}
          curl -S -s -o /dev/null http://${{ env.APP_ENDPOINT }}/client-call

      # Validation for app signals telemetry data
      - name: Call endpoint and validate generated EMF logs
        id: log-validation
        if: steps.deploy-python-app.outcome == 'success' && !cancelled()
        run: ./gradlew validator:run --args='-c python/eks/log-validation.yml
          --testing-id ${{ env.TESTING_ID }}
          --endpoint http://${{ env.APP_ENDPOINT }}
          --region ${{ inputs.aws-region }}
          --account-id ${{ env.ACCOUNT_ID }}
          --metric-namespace ${{ env.METRIC_NAMESPACE }}
          --log-group ${{ env.LOG_GROUP_NAME }}
          --app-namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}
          --platform-info ${{ inputs.test-cluster-name }}
          --service-name python-application-${{ env.TESTING_ID }}
          --remote-service-deployment-name ${{ env.REMOTE_SERVICE_DEPLOYMENT_NAME }}
          --request-body ip=${{ env.REMOTE_SERVICE_POD_IP }}
          --rollup'

      - name: Call endpoints and validate generated metrics
        id: metric-validation
        if: (steps.deploy-python-app.outcome == 'success' || steps.log-validation.outcome == 'failure') && !cancelled()
        run: ./gradlew validator:run --args='-c python/eks/metric-validation.yml
          --testing-id ${{ env.TESTING_ID }}
          --endpoint http://${{ env.APP_ENDPOINT }}
          --region ${{ inputs.aws-region }}
          --account-id ${{ env.ACCOUNT_ID }}
          --metric-namespace ${{ env.METRIC_NAMESPACE }}
          --log-group ${{ env.LOG_GROUP_NAME }}
          --app-namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}
          --platform-info ${{ inputs.test-cluster-name }}
          --service-name python-application-${{ env.TESTING_ID }}
          --remote-service-name python-remote-application-${{ env.TESTING_ID }}
          --remote-service-deployment-name ${{ env.REMOTE_SERVICE_DEPLOYMENT_NAME }}
          --request-body ip=${{ env.REMOTE_SERVICE_POD_IP }}
          --rollup'

      - name: Call endpoints and validate generated traces
        id: trace-validation
        if: (steps.deploy-python-app.outcome == 'success' || steps.log-validation.outcome == 'failure' || steps.metric-validation.outcome == 'failure') && !cancelled()
        run: ./gradlew validator:run --args='-c python/eks/trace-validation.yml
          --testing-id ${{ env.TESTING_ID }}
          --endpoint http://${{ env.APP_ENDPOINT }}
          --region ${{ inputs.aws-region }}
          --account-id ${{ env.ACCOUNT_ID }}
          --metric-namespace ${{ env.METRIC_NAMESPACE }}
          --log-group ${{ env.LOG_GROUP_NAME }}
          --app-namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}
          --platform-info ${{ inputs.test-cluster-name }}
          --service-name python-application-${{ env.TESTING_ID }}
          --remote-service-deployment-name ${{ env.REMOTE_SERVICE_DEPLOYMENT_NAME }}
          --request-body ip=${{ env.REMOTE_SERVICE_POD_IP }}
          --rollup'

      - name: Publish metric on test result
        if: always()
        run: |
          if [[ "${{ steps.log-validation.outcome }}" == "success" && "${{ steps.metric-validation.outcome }}" == "success" && "${{ steps.trace-validation.outcome }}" == "success" ]]; then
            aws cloudwatch put-metric-data --namespace 'ADOT/GitHubActions' \
            --metric-name Failure \
            --dimensions repository=${{ github.repository }},branch=${{ github.ref_name }},workflow=${{ inputs.caller-workflow-name }} \
            --value 0.0 \
            --region ${{ inputs.aws-region }}
          else
            aws cloudwatch put-metric-data --namespace 'ADOT/GitHubActions' \
            --metric-name Failure \
            --dimensions repository=${{ github.repository }},branch=${{ github.ref_name }},workflow=${{ inputs.caller-workflow-name }} \
            --value 1.0 \
            --region ${{ inputs.aws-region }}
          fi

      # Clean up Procedures

      - name: Clean Up App Signals
        if: always()
        continue-on-error: true
        working-directory: amazon-cloudwatch-agent-operator/
        run: |
          kubectl delete -f ./namespace.yaml

      # This step also deletes lingering resources from previous test runs
      - name: Delete all sample app resources
        if: always()
        continue-on-error: true
        timeout-minutes: 10
        run: kubectl delete namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}

      - name: Terraform destroy
        if: always()
        continue-on-error: true
        working-directory: terraform/python/eks
        run: |
          terraform destroy -auto-approve \
            -var="test_id=${{ env.TESTING_ID }}" \
            -var="aws_region=${{ inputs.aws-region }}" \
            -var="kube_directory_path=${{ github.workspace }}/.kube" \
            -var="eks_cluster_name=${{ inputs.test-cluster-name }}" \
            -var="test_namespace=${{ env.PYTHON_SAMPLE_APP_NAMESPACE }}" \
            -var="service_account_aws_access=service-account-${{ env.TESTING_ID }}" \
            -var="python_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_FE_SA_IMG }}" \
            -var="python_remote_app_image=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ inputs.aws-region }}.amazonaws.com/${{ secrets.APP_SIGNALS_PYTHON_E2E_RE_SA_IMG }}"

      - name: Remove aws access service account
        if: always()
        continue-on-error: true
        run: |
          eksctl delete iamserviceaccount \
          --name service-account-${{ env.TESTING_ID }} \
          --namespace ${{ env.PYTHON_SAMPLE_APP_NAMESPACE }} \
          --cluster ${{ inputs.test-cluster-name }} \
          --region ${{ inputs.aws-region }}
